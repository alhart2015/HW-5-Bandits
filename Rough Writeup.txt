Abstract:

The multi-armed bandit problem confronts the exploitation-versus-exploitation dilemma. In situations where you have known rewards and unknown options, a decision must be made whether to play it safe with the known quantities, or explore the other options, possibly uncovering a greater reward. We examine two algorithms for striking a balance between the two, UCB-1 and $\epsilon$-Greedy. We show that SOMETHING ABOUT UCB and that the $\epsilon$-Greedy algorithm produces a strategy for nearly perfect play.

Introduction:

The classic application of the multi-armed bandit is the slot machine problem: You are sitting at a slot machine that pays you $1 60% of the time. Do you play it safe and continue to earn a dollar most of the time? Or do you explore the other machines around you with hopes of a bigger payout? This kind of exploration-versus-exploitation problem has applications in many other fields as well, notably graphic design, game-playing, and medical research. In this experiment, we explore two algorithms to address the issue, UCB-1 and $\epsilon$-Greedy. Both algorithms attempt to explore the possible actions to determine the best one, while also playing the suspected optimal move as frequently as possible.

Experiment:

Another policy for playing the optimal machine was the $\epsilon$-greedy algorithm. At each play, the policy determines an $\epsilon$ value, which indicates the confidence that the machine with the highest current average reward is actually the optimal machine. Naturally, as more data is gathered, that confidence increases. For the nth play, n = 1, 2, 3, ..., we let e_n = min{1, cK/d^2n} where c and d are user-defined parameters, K = the number of machines, and n is the number of plays to this point. In our experiment, set c to be 5 and d to be 0.5. Auer et. al state that a large enough c gives a strong bound on the instantaneous regret. They offer c=5 as one such large c, hence our selection of the value of c. The d parameter, 0 < d < 1, adjusts the speed at which you gain confidence. A larger d means you are more confident quicker, and vice-versa. To give a reasonable value for d, Auer et. al suggest choosing 0 < d <= min(deltai) where deltai = u* - ui. Note that ui is the reward expectation for machine i and u* is any maximal element of the set {u1, ..., uK}.

Thus with these parameters set, we ran the algorithm for 10^5 plays. At each play, we calculated the $\epsilon$ value for that play, en. With probability 1-en, we played the machine with the highest current average reward, and with probability en we played a random machine. This allowed us to continually add noise to our model, which prevents convergence on a local maximum. As more plays are made, the value of en decreases, leading to a more likely play of the estimated best machine. We call the algorithm greedy because it only ever considers playing the estimated best machine, or a random one.

Results:

We impleented the UCB-1 and $\epsilon$-Greedy bandits on each of the following distributions:

Distribution    Machine
                1       2       3       4       5       6       7       8       9       10
1               0.9     0.6
3               0.55    0.45
11              0.9     0.6     0.6     0.6     0.6     0.6     0.6     0.6     0.6     0.6
14              0.55    0.45    0.45    0.45    0.45    0.45    0.45    0.45    0.45    0.45

The $\epsilon$-greedy algorithm converged very quickly to the optimal machine. Over 10^5 runs, the bandit produced the following results:

Bandit  Percentage of Optimal Plays     Cumulative Regret
1       99.839                          360.0
3       99.818                          374.0
11      98.675                          1481.0
14      98.66                           1505.0

We see that in every case, the bandit played the optimal machine the overwhelming majority of the time. The cumulative regret is higher for bandits 11 and 14, as those had 10 machines to choose from, and thus took longer to converge to the correct machine. However, as we see from the graphs, the bandits did converge to the optimal machine and played it better than 98% of the time. By decreasing the value of c, we can achieve nearly perfect results. Auer et. al. tested values  of c = 0.15, 0.10, and 0.05. We replicated these results, and saw that the smallest c values lead to outstanding results. The following were produced with c = 0.05:

Bandit  Percentage of Optimal Plays     Cumulative Regret
1       99.999                          3.0
3       99.998                          5.0
11      98.976                          26.0
14      98.98                           23.0

These results clearly demonstrate a near-perfect playing strategy with a small c value.

In both the UCB-1 and $\epsilon$-Greedy cases, we see that Bandits 3 and 14 had a harder time converging than Bandits 1 and 11. This is due to the underlying distributions being played. Bandits 1 and 11 played distributions with a clear best choice, while 3 and 14 had much more uniform choices to choose from. This made it harder to determine which was the best choice, but ultimately both algorithms were able to do so.