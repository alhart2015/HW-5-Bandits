Abstract:

The multi-armed bandit problem confronts the exploitation-versus-exploitation dilemma. In situations where you have known rewards and unknown options, a decision must be made whether to play it safe with the known quantities, or explore the other options, possibly uncovering a greater reward. We examine two algorithms for striking a balance between the two, UCB-1 and $\epsilon$-Greedy. We show that SOMETHING ABOUT UCB and that the $\epsilon$-Greedy algorithm produces a strategy for nearly perfect play.

Introduction:

The classic application of the multi-armed bandit is the slot machine problem: You are sitting at a slot machine that pays you $1 60% of the time. Do you play it safe and continue to earn a dollar most of the time? Or do you explore the other machines around you with hopes of a bigger payout? This kind of exploration-versus-exploitation problem has applications in many other fields as well, notably graphic design, game-playing, and medical research. In this experiment, we explore two algorithms to address the issue, UCB-1 and $\epsilon$-Greedy. Both algorithms attempt to explore the possible actions to determine the best one, while also playing the suspected optimal move as frequently as possible.

Experiment:
One policy that we used to implement the k-armed bandit problem was the UCB1 bandit algorithm. The UCB1 bandit is initialized by playing each machine once. This initializes the expected returns for each machine, and gives a basis for selection going forward. For each future turn, the bandit chooses to play the machine that maximizes x_j_bar + sqrt((2*ln(n))/n_j), where x_j_bar represents the average return of machine j thus far, and the second term represents the upper confidence bound for the variance of machine j. The performance of the UCB1 bandit is measured by taking the ‘regret’ of that move, represented by mu_* - mu_j, where mu_* is the return that the bandit would have received had it played the optimal machine on that turn and mu_j is the return of the machine that the bandit chose. To track and evaluate the UCB1 bandit’s performance over time, we keep track of the cumulative regret over time and the percentage of times that the bandit plays the optimal machine. The regret of the UCB1 bandit after n plays is defined by u*n- u_jsummation(1 to k IE[Tj(n)] where u* def = max u_i and IE[] denotes expectation. Our goal is to minimize the cumulative regret, which we can do by minimizing the amount of time before the bandit converges on the optimal machine. We kept track father bandit’s behavior over 10^5 rounds. 

Another policy for playing the optimal machine was the epsilon-greedy algorithm. At each play, the policy determines an epsilon value, which indicates the confidence that the machine with the highest current average reward is actually the optimal machine. Naturally, as more data is gathered, that confidence increases. For the nth play, n = 1, 2, 3, ..., we let e_n = min{1, cK/d^2n} where c and d are user-defined parameters, K = the number of machines, and n is the number of plays to this point. In our experiment, set c to be 5 and d to be 0.5. Auer et. al state that a large enough c gives a strong bound on the instantaneous regret. They offer c=5 as one such large c, hence our selection of the value of c. The d parameter, 0 < d < 1, adjusts the speed at which you gain confidence. A larger d means you are more confident quicker, and vice-versa. To give a reasonable value for d, Auer et. al suggest choosing 0 < d <= min(deltai) where deltai = u* - ui. Note that ui is the reward expectation for machine i and u* is any maximal element of the set {u1, ..., uK}.

Thus with these parameters set, we ran the algorithm for 10^5 plays. At each play, we calculated the epsilon value for that play, en. With probability 1-en, we played the machine with the highest current average reward, and with probability en we played a random machine. This allowed us to continually add noise to our model, which prevents convergence on a local maximum. As more plays are made, the value of en decreases, leading to a more likely play of the estimated best machine. We call the algorithm greedy because it only ever considers playing the estimated best machine, or a random one.


Results:

We implemented the UCB-1 and $\epsilon$-Greedy bandits on each of the following distributions:

Distribution    Machine
                1       2       3       4       5       6       7       8       9       10
1               0.9     0.6
3               0.55    0.45
11              0.9     0.6     0.6     0.6     0.6     0.6     0.6     0.6     0.6     0.6
14              0.55    0.45    0.45    0.45    0.45    0.45    0.45    0.45    0.45    0.45


Over 10^5 runs, the UCB1 bandit produced the following results:

Bandit  Percentage of Optimal Plays     Cumulative Regret
1       99.738                          80.0
3       97.946                          189.0
11      97.993                          618.0
14      87.374                          1330.0

We see that in every case, the bandit converges on the optimal machine in logarithmic time. As we can see in the figure below:

Bandit 1 and Bandit 2, the 2-armed bandits, reach a much lower maximum cumulative regret than bandits 11 and 14, the 10-armed bandits. We can see that as the bandit is given more machines to choose from, it takes longer to converge to the optimal machine. All four bandits played the best machine an overwhelming majority of the time, with bandits 1, 3, and 11 playing the optimal strategy over 97% of the time, while Bandit 14 came in just under 90% at around 87%.

These results show that the UCB1 bandit algorithm works very quickly on reward distributions with fewer machines, but takes longer as the number of machines, n, increases.

The $\epsilon$-greedy algorithm converged very quickly to the optimal machine. Over 10^5 runs, the bandit produced the following results:

Bandit  Percentage of Optimal Plays     Cumulative Regret
1       99.839                          360.0
3       99.818                          374.0
11      98.675                          1481.0
14      98.66                           1505.0

We see that in every case, the bandit played the optimal machine the overwhelming majority of the time. The cumulative regret is higher for bandits 11 and 14, as those had 10 machines to choose from, and thus took longer to converge to the correct machine. However, as we see from the graphs, the bandits did converge to the optimal machine and played it better than 98% of the time. By decreasing the value of c, we can achieve nearly perfect results. Auer et. al. tested values  of c = 0.15, 0.10, and 0.05. We replicated these results, and saw that the smallest c values lead to outstanding results. The following were produced with c = 0.05:

Bandit  Percentage of Optimal Plays     Cumulative Regret
1       99.999                          3.0
3       99.998                          5.0
11      98.976                          26.0
14      98.98                           23.0

These results clearly demonstrate a near-perfect playing strategy with a small c value.

In both the UCB-1 and $\epsilon$-Greedy cases, we see that Bandits 3 and 14 had a harder time converging than Bandits 1 and 11. This is due to the underlying distributions being played. Bandits 1 and 11 played distributions with a clear best choice, while 3 and 14 had much more uniform choices to choose from. This made it harder to determine which was the best choice, but ultimately both algorithms were able to do so.